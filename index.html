<!DOCTYPE html><html lang=en><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><title>Two-Stream Convolutional Networks for Dynamic Texture Synthesis</title><meta name=description content="The project page for the paper &quot;Two-Stream Convolutional Networks for Dynamic Texture Synthesis&quot;"><style type=text/css>body,h2,h6,p{margin:0;padding:0}body{font:400 16px/1.5 "Helvetica Neue",Helvetica,Arial,sans-serif;color:#515151;background-color:#fff;-webkit-text-size-adjust:100%;-o-font-feature-settings:"kern" 1;font-feature-settings:"kern" 1;font-kerning:normal}h2,h6,p{margin-bottom:15px}h2,h6{font-weight:400}.wrapper{max-width:calc(800px - (30px * 2));margin-right:auto;margin-left:auto;padding-right:30px;padding-left:30px}@media screen and (max-width:800px){.wrapper{max-width:calc(800px - (30px));padding-right:15px;padding-left:15px}}.abstract-wrapper:after,.wrapper:after{content:"";display:table;clear:both}.page-content{padding-bottom:30px}.page-header{text-align:center;border-top:5px solid #c698cb;border-bottom:1px solid #e8e8e8;padding:0 30px;font-size:20px}.page-header h2{margin-bottom:0}.header{margin-top:30px;margin-bottom:30px;border-bottom:5px solid #c698cb}.abstract-wrapper p{margin:0;text-align:justify}</style><script id=loadcss>!function(e,n){!function(e){"use strict";var n=function(n,t,o){var l,i=e.document,r=i.createElement("link");if(t)l=t;else{var a=(i.body||i.getElementsByTagName("head")[0]).childNodes;l=a[a.length-1]}var d=i.styleSheets;r.rel="stylesheet",r.href=n,r.media="only x",l.parentNode.insertBefore(r,t?l:l.nextSibling);var s=function(e){for(var n=r.href,t=d.length;t--;)if(d[t].href===n)return e();setTimeout(function(){s(e)})};return r.onloadcssdefined=s,s(function(){r.media=o||"all"}),r};"undefined"!=typeof module?module.exports=n:e.loadCSS=n}("undefined"!=typeof global?global:this);for(var t in e)loadCSS(e[t],n)}(["css/main.css"],document.getElementById("loadcss"))</script><noscript><link rel=stylesheet href=css/main.css></noscript><link rel=canonical href=https://ryersonvisionlab.github.io/two-stream-projpage// ><link rel=alternate type=application/rss+xml title="Two-Stream Convolutional Networks for Dynamic Texture Synthesis" href=feed.xml><link rel=apple-touch-icon sizes=180x180 href=favicons/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=favicons/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=favicons/favicon-16x16.png><link rel=manifest href=favicons/manifest.json><link rel=mask-icon href=favicons/safari-pinned-tab.svg color=#669c35><meta name=theme-color content=#ffffff><script>!function(e,t,a,n,c,o,s){e.GoogleAnalyticsObject=c,e[c]=e[c]||function(){(e[c].q=e[c].q||[]).push(arguments)},e[c].l=1*new Date,o=t.createElement(a),s=t.getElementsByTagName(a)[0],o.async=1,o.src="https://www.google-analytics.com/analytics.js",s.parentNode.insertBefore(o,s)}(window,document,"script",0,"ga"),ga("create","UA-74037692-2","auto"),ga("send","pageview")</script></head><body><header class=page-header><h2 class=page-title>Two-Stream Convolutional Networks for Dynamic Texture Synthesis</h2><h6><em>Under review for NIPS 2017</em><h6></h6></h6></header><main class=page-content aria-label=Content><div class=wrapper><div class=home><div class=home-content><div class=abstract-wrapper><h2 class="header abstract-header">Abstract</h2><p>We introduce a two-stream model for dynamic texture synthesis. Our model is based on pre-trained convolutional networks (ConvNets) that target two independent tasks: (i) object recognition, and (ii) optical flow prediction. Given an input dynamic texture, statistics of filter responses from the object recognition ConvNet encapsulates the per frame appearance of the input texture, while statistics of filter responses from the optical flow ConvNet models its dynamics. To generate a novel texture, a noise input sequence is optimized to simultaneously match the feature statistics from each stream of the example texture. Inspired by recent work on image style transfer and enabled by the two-stream model, we also apply the synthesis approach to combine the texture appearance from one texture with the dynamics of another to generate entirely novel dynamic textures. We show that our approach generates novel, high quality samples that match both the framewise appearance and temporal evolution of an input example.</p></div><div class=material-wrapper><h2 class="header material-header">Material</h2><div class=pdf><a href=pdf/twostreamconv.pdf><img src=img/twostreamconv_preview.jpg alt="Two-Stream Convolutional Networks for Dynamic Texture Synthesis"><span>Paper</span></a></div><div class=pdf><a href=pdf/twostreamconvposter.pdf><img src=img/twostreamconvposter_preview.jpg alt="Poster for Two-Stream Convolutional Networks for Dynamic Texture Synthesis"><span>Poster</span></a></div><div class=pdf><a href=#><span class=icon><svg viewbox="0 0 16 16"><path fill=#828282 d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761 c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32 c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472 c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037 C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65 c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261 c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082 c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129 c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/></svg> </span><span>Code (soon)</span></a></div></div><div class=author-col-wrapper><h2 class="header author-header">Authors</h2><div class="author-col author-col-1"><a href=http://mtesfaldet.net/ ><img class=profile src=img/mat.jpg> <span>Matthew Tesfaldet</span></a><ul class=social-media-list><li><a href=https://github.com/tesfaldet><span class=icon><svg viewbox="0 0 16 16"><path fill=#828282 d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761 c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32 c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472 c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037 C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65 c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261 c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082 c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129 c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/></svg></span></a></li><li><a href=https://twitter.com/mtesfald><span class=icon><svg viewbox="0 0 16 16"><path d="M15.969,3.058c-0.586,0.26-1.217,0.436-1.878,0.515c0.675-0.405,1.194-1.045,1.438-1.809c-0.632,0.375-1.332,0.647-2.076,0.793c-0.596-0.636-1.446-1.033-2.387-1.033c-1.806,0-3.27,1.464-3.27,3.27 c0,0.256,0.029,0.506,0.085,0.745C5.163,5.404,2.753,4.102,1.14,2.124C0.859,2.607,0.698,3.168,0.698,3.767 c0,1.134,0.577,2.135,1.455,2.722C1.616,6.472,1.112,6.325,0.671,6.08c0,0.014,0,0.027,0,0.041c0,1.584,1.127,2.906,2.623,3.206 C3.02,9.402,2.731,9.442,2.433,9.442c-0.211,0-0.416-0.021-0.615-0.059c0.416,1.299,1.624,2.245,3.055,2.271 c-1.119,0.877-2.529,1.4-4.061,1.4c-0.264,0-0.524-0.015-0.78-0.046c1.447,0.928,3.166,1.469,5.013,1.469 c6.015,0,9.304-4.983,9.304-9.304c0-0.142-0.003-0.283-0.009-0.423C14.976,4.29,15.531,3.714,15.969,3.058z"/></svg></span></a></li><li><a href=mailto:mtesfald@eecs.yorku.ca><span class=icon><svg viewbox="0 0 14 16"><path d="M0,4 L0,12 C0,12.55 0.45,13 1,13 L13,13 C13.55,13 14,12.55 14,12 L14,4 C14,3.45 13.55,3 13,3 L1,3 C0.45,3 0,3.45 0,4 L0,4 Z M13,4 L7,9 L1,4 L13,4 L13,4 Z M1,5.5 L5,8.5 L1,11.5 L1,5.5 L1,5.5 Z M2,12 L5.5,9 L7,10.5 L8.5,9 L12,12 L2,12 L2,12 Z M13,11.5 L9,8.5 L13,5.5 L13,11.5 L13,11.5 Z"/></svg></span></a></li></ul></div><div class="author-col author-col-2"><a href=http://www.scs.ryerson.ca/~kosta/ ><img class=profile src=img/kosta.jpg> <span>Konstantinos G. Derpanis</span></a><ul class=social-media-list><li><a href=https://twitter.com/CSProfKGD><span class=icon><svg viewbox="0 0 16 16"><path d="M15.969,3.058c-0.586,0.26-1.217,0.436-1.878,0.515c0.675-0.405,1.194-1.045,1.438-1.809c-0.632,0.375-1.332,0.647-2.076,0.793c-0.596-0.636-1.446-1.033-2.387-1.033c-1.806,0-3.27,1.464-3.27,3.27 c0,0.256,0.029,0.506,0.085,0.745C5.163,5.404,2.753,4.102,1.14,2.124C0.859,2.607,0.698,3.168,0.698,3.767 c0,1.134,0.577,2.135,1.455,2.722C1.616,6.472,1.112,6.325,0.671,6.08c0,0.014,0,0.027,0,0.041c0,1.584,1.127,2.906,2.623,3.206 C3.02,9.402,2.731,9.442,2.433,9.442c-0.211,0-0.416-0.021-0.615-0.059c0.416,1.299,1.624,2.245,3.055,2.271 c-1.119,0.877-2.529,1.4-4.061,1.4c-0.264,0-0.524-0.015-0.78-0.046c1.447,0.928,3.166,1.469,5.013,1.469 c6.015,0,9.304-4.983,9.304-9.304c0-0.142-0.003-0.283-0.009-0.423C14.976,4.29,15.531,3.714,15.969,3.058z"/></svg></span></a></li><li><a href=mailto:kosta@scs.ryerson.ca><span class=icon><svg viewbox="0 0 14 16"><path d="M0,4 L0,12 C0,12.55 0.45,13 1,13 L13,13 C13.55,13 14,12.55 14,12 L14,4 C14,3.45 13.55,3 13,3 L1,3 C0.45,3 0,3.45 0,4 L0,4 Z M13,4 L7,9 L1,4 L13,4 L13,4 Z M1,5.5 L5,8.5 L1,11.5 L1,5.5 L1,5.5 Z M2,12 L5.5,9 L7,10.5 L8.5,9 L12,12 L2,12 L2,12 Z M13,11.5 L9,8.5 L13,5.5 L13,11.5 L13,11.5 Z"/></svg></span></a></li></ul></div><div class="author-col author-col-3"><a href=http://www.eecs.yorku.ca/~mab/ ><img class=profile src=img/mab.png> <span>Marcus A. Brubaker</span></a><ul class=social-media-list><li><a href=https://github.com/mbrubake><span class=icon><svg viewbox="0 0 16 16"><path fill=#828282 d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761 c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32 c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472 c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037 C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65 c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261 c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082 c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129 c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/></svg></span></a></li><li><a href=https://twitter.com/marcusabrubaker><span class=icon><svg viewbox="0 0 16 16"><path d="M15.969,3.058c-0.586,0.26-1.217,0.436-1.878,0.515c0.675-0.405,1.194-1.045,1.438-1.809c-0.632,0.375-1.332,0.647-2.076,0.793c-0.596-0.636-1.446-1.033-2.387-1.033c-1.806,0-3.27,1.464-3.27,3.27 c0,0.256,0.029,0.506,0.085,0.745C5.163,5.404,2.753,4.102,1.14,2.124C0.859,2.607,0.698,3.168,0.698,3.767 c0,1.134,0.577,2.135,1.455,2.722C1.616,6.472,1.112,6.325,0.671,6.08c0,0.014,0,0.027,0,0.041c0,1.584,1.127,2.906,2.623,3.206 C3.02,9.402,2.731,9.442,2.433,9.442c-0.211,0-0.416-0.021-0.615-0.059c0.416,1.299,1.624,2.245,3.055,2.271 c-1.119,0.877-2.529,1.4-4.061,1.4c-0.264,0-0.524-0.015-0.78-0.046c1.447,0.928,3.166,1.469,5.013,1.469 c6.015,0,9.304-4.983,9.304-9.304c0-0.142-0.003-0.283-0.009-0.423C14.976,4.29,15.531,3.714,15.969,3.058z"/></svg></span></a></li><li><a href=mailto:mab@eecs.yorku.ca><span class=icon><svg viewbox="0 0 14 16"><path d="M0,4 L0,12 C0,12.55 0.45,13 1,13 L13,13 C13.55,13 14,12.55 14,12 L14,4 C14,3.45 13.55,3 13,3 L1,3 C0.45,3 0,3.45 0,4 L0,4 Z M13,4 L7,9 L1,4 L13,4 L13,4 Z M1,5.5 L5,8.5 L1,11.5 L1,5.5 L1,5.5 Z M2,12 L5.5,9 L7,10.5 L8.5,9 L12,12 L2,12 L2,12 Z M13,11.5 L9,8.5 L13,5.5 L13,11.5 L13,11.5 Z"/></svg></span></a></li></ul></div></div><div class=dyntexsynth-wrapper><h2 class="header dyntexsynth-header">Dynamic Texture Synthesis</h2></div><div class=dynstytrans-wrapper><h2 class="header dynstytrans-header">Dynamics Style Transfer</h2></div><div class=citation-wrapper><h2 class="header citation-header">Citation</h2><div class=highlighter-rouge><pre class=highlight><code>Matthew Tesfaldet, Konstantinos G. Derpanis, and Marcus A. Brubaker. Two-Stream Convolutional Networks for Dynamic Texture Synthesis. Unpublished, 2017.
</code></pre></div><p>Bibtex format:</p><div class=highlighter-rouge><pre class=highlight><code>@unpublished{tesfaldet2017,
  title = {Two-Stream Convolutional Networks for Dynamic Texture Synthesis},
  author = {Tesfaldet, Matthew and
            Derpanis, Konstantinos G. and
            Brubaker, Marcus A.},
  year = {2017},
  note = {Unpublished paper}
}
</code></pre></div></div></div></div></div></main><footer class=site-footer><div class=wrapper><div id=disqus_thread></div><script>var disqus_config=function(){this.page.url="https://ryersonvisionlab.github.io/two-stream-projpage//https://ryersonvisionlab.github.io/two-stream-projpage/",this.page.identifier="https://ryersonvisionlab.github.io/two-stream-projpage//https://ryersonvisionlab.github.io/two-stream-projpage/"};!function(){var t=document,e=t.createElement("script");e.src="https://two-stream-dyntex-synth.disqus.com/embed.js",e.setAttribute("data-timestamp",+new Date),(t.head||t.body).appendChild(e)}()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript rel=nofollow>comments powered by Disqus.</a></noscript></div></footer></body></html>